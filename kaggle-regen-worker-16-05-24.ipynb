{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ReGen Worker - Kaggle Branch\n### Remember to first set your API KEY and worker name\n#### You can serve different models, simply change the name(s) in models_to_load to match the model(s) you want, you can check either https://aqualxx.github.io/stable-ui/workers, in the models tab, https://tinybots.net/artbot/info/models, or https://aihorde.sitew3.com/#/models, just copy paste the name (it's set to AlbedoBase XL (SDXL) by default).\n#### You can also change max_power and see how high you can go, it's set to 32 by default","metadata":{"id":"gxTapT5ROAv1"}},{"cell_type":"code","source":"#0.- This cell will set the variables and rerun the worker if you stopped it, but only if everything else was installed already\n\n#### For right now, THESE are the only variables that we care about\nworker_name = \"\"\napi_key = \"\"\ncivitai_token = \"cae554bcc138d97a9323856c2dee1158\"\nmax_power = 32\nmax_batch = \"5\"\n\n# Set the model(s) down here\nmodels_to_load = [\"AlbedoBase XL (SDXL)\"]\nqueue_size = 2\nmax_threads = 1\n\n# I have not tested how this works at all, I'm just putting it together from the old notebook\n# The 2 gpus version DOES NOT support batches, do not ask me about it\n# I don't know if/how it will work with outside models, best not to try it\ngpu_2x_version = False\nif (gpu_2x_version):\n    # if using the 2 gpus version\n    models_to_load = [\"AlbedoBase XL (SDXL)\", \"Anything Diffusion\"]\n    queue_size = 3\n    max_threads = 2\n\n\nsafety_on_gpu = True\nallow_lora = True\nnsfw = True\ncensor_nsfw = False\n####For right now, THESE are the only variables that we care about\n\n#these are not used yet\nauto_dual_gpu = True\nmodels_to_pin = [\"AlbedoBase XL (SDXL)\", \"Anything Diffusion\"]\ngpus_to_use = [0, 1]\n\nhorde_url = \"https://aihorde.net\"\nallow_painting = False\ndynamic_models = False\nmodels_to_skip = [\"stable_diffusion_inpainting\", \"stable_diffusion_2.1\",  \"stable_diffusion_2.0\"]\nallow_post_processing = False\npriority_usernames = []\nblacklist = []\ncensorlist = []\nallow_img2img = True\nallow_controlnet = False\nallow_unsafe_ip = True\nnumber_of_dynamic_models = 0\nmax_models_to_download = 10\nforms = [\"caption\",\"nsfw\",\"interrogation\",\"post-process\"]\n\n\n# To use unsupported/deprecated models (Segmind, Van Gogh Diffusion, etc)\n# You need to set this next variable to True if using this, otherwise make sure it's False\noutside_model = False\n# The name of the model you are going to use, this is only so you know what model it is\noutside_model_name = \"Pony V6, SD 1.5\"\n# Part of the file name when downloaded; can't remember if it's actually necessary after everything I changed, but I can't be bothered to check/remove it\nmodel_file_name = \"pony\"\n# The baseline of the model you are using, 1.5 or XL\nbaseline = \"SD 1.5\" # [\"SD 1.5\", \"SD XL\"]\n# The download url, tested for both huggingface and civitai, so it should be fine\noutside_model_download_url = \"https://civitai.com/api/download/models/297064?type=Model&format=SafeTensor&size=pruned&fp=fp16\"\n\nif(\"PickleTensor\" in outside_model_download_url or \".ckpt\" in outside_model_download_url):\n    file_extension = \".ckpt\"\nif(\"SafeTensor\" in outside_model_download_url or \".safetensors\" in outside_model_download_url):\n    file_extension = \".safetensors\"\n\ncivitai = \"civitai.com/api/download/models\"\nif (civitai in outside_model_download_url):\n    outside_model_download_url = outside_model_download_url.split(\"?\")[0] + \"?token=\" + civitai_token\n\nmask15_ckpt = {\n    'model_name': \"Anything Diffusion\",\n    'file_name': \"Anything-Diffusion.ckpt\",\n    'sha256': \"633c153d96230355efb4230da6ae2e3ba85b084b93c89eb88cb1118d6cc06cef *Anything-Diffusion.sha256\",\n    'sha_name': \"Anything-Diffusion.sha256\"\n}\nmask15_safe = {\n    'model_name': \"Dreamshaper\",\n    'file_name': \"Dreamshaper.safetensors\",\n    'sha256': \"879db523c30d3b9017143d56705015e15a2cb5628762c11d086fed9538abd7fd *Dreamshaper.sha256\",\n    'sha_name': \"Dreamshaper.sha256\"\n}\nmaskxl_safe = {\n    'model_name': \"AlbedoBase XL (SDXL)\",\n    'file_name': \"albedo_base_xl.safetensors\",\n    'sha256': \"1718B5BB2DA1EF4815FEE8AF8A7FC2FA8AB8F467B279EDED4D991EA0CCE59A6D *albedo_base_xl.sha256\",\n    'sha_name': \"albedo_base_xl.sha256\"\n}\nif (baseline == \"SD XL\"):\n    mask = maskxl_safe\nif (baseline == \"SD 1.5\"):\n    if(file_extension == \".ckpt\"):\n        mask = mask15_ckpt\n    if(file_extension == \".safetensors\"):\n        mask = mask15_safe\n\nif (outside_model and outside_model_name and outside_model_download_url):\n    models_to_load = []\n    models_to_load = [mask['model_name']]\n    print (\"Running model not supported by the Horde (non-Customizer Role)\")\n    print (f'''Model: {outside_model_name:}, Mask: {mask['model_name']}, format: {mask['file_name'].split(\".\")[1]}''')\n# To use unsupported/deprecated models (Segmind, Van Gogh Diffusion, etc)\n\n\nselected_models = set(list(models_to_load))\nprint (f'Running: {models_to_load}')\n\n\ncurrent_path = \"/kaggle/working/\"\nworker_path = current_path + \"horde-worker-reGen/\"\nbridgeData_file = worker_path + \"bridgeData.yaml\"\n\n# To toggle Maintenance Mode ON/OFF\n# Maintenance Mode will be turned ON automatically if using outside models\nMaintenance_Mode = False\n\n\nimport os\n\nif not os.path.exists(bridgeData_file):\n    print (\"bridgeData.yaml file not found. Proceed to install the worker.\")\nelse:\n    print (\"bridgeData.yaml file found. Recreating bridgeData.yaml and restarting the worker.\")\n    create_yaml()\n    download_models_aria2c()\n\n    if (Maintenance_Mode):\n        Maintenance_Mode = True\n    else:\n        if (outside_model and outside_model_name and outside_model_download_url):\n            Maintenance_Mode = True\n        else:\n            Maintenance_Mode = False\n    Set_Maintenance_Mode_function(Maintenance_Mode)\n\n    #!cd /kaggle/working\n    #!source ../regen/bin/activate;python download_models.py\n    !cd /kaggle/working\n    !source ../regen/bin/activate;python run_worker.py -vv\n","metadata":{"scrolled":true,"id":"6jsYiM8TOAv5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the Worker OUT of Maintenance Mode\n#This cell is to get the worker out of maintenance before starting, since some people don't know how or where to do it\n\nimport subprocess\nimport json\nimport re\nimport requests\n\ndef Set_Maintenance_Mode_function(Maintenance_Mode):\n\n    i = False #Worker does not match/exist flag\n\n    command_find_user = f'''curl -X 'GET' \\\n              '{horde_url}/api/v2/find_user' \\\n              -H 'accept: application/json' \\\n              -H 'apikey: {api_key}' \\\n              -H 'Client-Agent: unknown:0:unknown'\n            '''\n\n    find_user_response = subprocess.check_output(command_find_user, shell=True).decode('utf-8')\n    data_api_key = json.loads(find_user_response)\n\n    if not (\"message\" in find_user_response):\n        for worker_id in data_api_key['worker_ids']:\n            #print(worker_id)\n\n            command_find_worker = f'''curl -X 'GET' \\\n                      '{horde_url}/api/v2/workers/{worker_id}' \\\n                      -H 'accept: application/json' \\\n                      -H 'apikey: {api_key}' \\\n                      -H 'Client-Agent: unknown:0:unknown'\n                    '''\n            find_worker_response = subprocess.check_output(command_find_worker, shell=True).decode('utf-8')\n            data_worker = json.loads(find_worker_response)\n            if (worker_name == data_worker['name']):\n                print(f'''Worker name: {worker_name}, Worker ID: {worker_id}''')\n                i = True\n\n                worker_uuid = worker_id\n\n                if (Maintenance_Mode):\n                  set_maintenance_mode = \"true\"\n                else:\n                  set_maintenance_mode = \"false\"\n\n                command_maintenance_mode = f'''curl -X 'PUT' \\\n                  '{horde_url}/api/v2/workers/{worker_id}' \\\n                  -H 'accept: application/json' \\\n                  -H 'apikey: {api_key}' \\\n                  -H 'Client-Agent: unknown:0:unknown' \\\n                  -H 'Content-Type: application/json' \\\n                  -d'''+ r''' '{\n                  \"maintenance\":''' + f''' {set_maintenance_mode}\n                ''' + r'''}'\n                '''\n\n                maintenance_mode_response = subprocess.check_output(command_maintenance_mode, shell=True).decode('utf-8')\n                print(maintenance_mode_response)\n\n        if not i:\n          print(f'''No worker with the name: {worker_name}, can't change Maintenance Mode. A new worker will be created with that name.''')\n          worker_id = 0\n    else:\n        print(f'''The API key \"{api_key}\" does not exist''')\n\nSet_Maintenance_Mode_function(Maintenance_Mode = False)\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#1.- Virtual Environment\n\n!python -m virtualenv regen\n","metadata":{"id":"u9IhYNmiOAv7","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2.- Remove the worker if it exists, then, Clone the regen worker\n\n!cd /kaggle/working;rm -r horde-worker-reGen\n\nif not (gpu_2x_version):\n    # if using the 1 gpu version\n    !cd /kaggle/working;git clone https://github.com/Haidra-Org/horde-worker-reGen.git\nelse:\n    # if using the 2 gpus version\n    !cd /kaggle/working;git clone https://github.com/Haidra-Org/horde-worker-reGen.git --branch kaggle --single-branch\n","metadata":{"id":"K9V8rghXOAv8","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3.- Install requirements\n\nif not (gpu_2x_version):\n    # if using the 1 gpu version\n    !source regen/bin/activate;pip install -r .\\/horde-worker-reGen/requirements.txt\nelse:\n    # if using the 2 gpus version\n    !source regen/bin/activate;pip install -r .\\/horde-worker-reGen/requirements.118.txt\n    %cd /kaggle/working/\n    !source regen/bin/activate;pip install -r .\\/horde-worker-reGen/requirements.hordelib.txt\n    !source regen/bin/activate;pip install hordelib==2.1.1 --no-deps\n","metadata":{"scrolled":true,"id":"8jDLCvV7OAv9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#4.- Create .yaml config file\n\nimport os\n\n%cd $worker_path\nprint (\"Creating bridgeData.yaml file.\")\n\ndef create_yaml():\n    \n    from yaml import load, dump\n\n    def make_yaml_sublist(list_to_convert: list[str]):\n        sublist_yaml = dump(list_to_convert)\n        sublist_yaml = \"\\n\" + sublist_yaml\n        return sublist_yaml\n\n\n    data = f\"\"\"horde_url: \"{horde_url}\"\napi_key: \"{api_key}\"\ncivitai_api_token: \"{civitai_token}\"\ngpus_to_use: {gpus_to_use}\npriority_usernames: []\nmax_threads: {max_threads}\nqueue_size: {queue_size}\nmax_batch: {max_batch}\nsafety_on_gpu: {safety_on_gpu}\n#auto_dual_gpu: {auto_dual_gpu}, no longer necessary, should be ignored like this\nrequire_upfront_kudos: false\ndreamer_name: \"{worker_name}\"\nmax_power: {max_power}\nnsfw: {nsfw.__str__().lower()}\ncensor_nsfw: {censor_nsfw}\nblacklist: {blacklist}\ncensorlist: {censorlist}\nallow_img2img: {allow_img2img.__str__().lower()}\nallow_painting: {allow_painting.__str__().lower()}\nallow_unsafe_ip: true\nallow_post_processing: {allow_post_processing.__str__().lower()}\nallow_controlnet: false\nallow_lora: {allow_lora.__str__().lower()}\nmax_lora_cache_size: 10\ndynamic_models: false\nnumber_of_dynamic_models: 0\nmax_models_to_download: 10\nstats_output_frequency: 30\ncache_home: \"./\"\nalways_download: true\ntemp_dir: \"./tmp\"\ndisable_terminal_ui: True\nvram_to_leave_free: \"80%\"\nram_to_leave_free: \"80%\"\ndisable_disk_cache: false\nmodels_to_load: {make_yaml_sublist(models_to_load)}\nmodels_to_pin: {make_yaml_sublist(models_to_load)}\nmodels_to_skip: {make_yaml_sublist(models_to_skip)}\nsuppress_speed_warnings: false\nforms:\n- \"caption\"\n- \"nsfw\"\n- \"interrogation\"\n- \"post-process\"\n\"\"\"\n\n    with open(bridgeData_file, \"w\") as text_file:\n        text_file.write(data)\n    \n    print (\"bridgeData.yaml file created.\")\n\ncreate_yaml()\n","metadata":{"id":"Imn0iDzHOAv-","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#5.- Symlinks for models and loras\n\n!mkdir /kaggle/tmp\n!mkdir /kaggle/tmp/compvis\n!mkdir /kaggle/tmp/lora\n\n\n!ln -s /kaggle/tmp/compvis/ /kaggle/working/horde-worker-reGen/compvis\n!ln -s /kaggle/tmp/lora/ /kaggle/working/horde-worker-reGen/lora\n","metadata":{"id":"nXCAwPVFOAv_","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download all models with aria2c\n\nmodel_reference = \"/kaggle/working/stable_diffusion.json\"\n\n# download the model reference if necessary\nimport os\nif os.path.exists(model_reference):\n    print(\"Model Reference exists.\")\nelse:\n    print(\"Downloading Model Reference.\")\n    !wget https://raw.githubusercontent.com/Haidra-Org/AI-Horde-image-model-reference/main/stable_diffusion.json -O /kaggle/working/stable_diffusion.json\n\n# install aria2c if necessary\nimport subprocess\ntry:\n    # Run the command\n    output = subprocess.check_output([\"which\", \"aria2c\"])\n    # unnecessary to do this next step, but whatever, I can use the output from above with this\n    output = output.decode(\"utf-8\")\n    print(\"aria2c is installed.\")\nexcept:\n    print(f\"Installing aria2c.\")\n    !apt-get install -y aria2\n\n\n\n# function to look in the model reference for the selected models\nimport json\ndef find_matching_entries(filename, key, value_list):\n    \"\"\"\n    Finds all entries in a JSON file that match a given key and value list.\n\n    Args:\n      filename: The path to the JSON file.\n      key: The key to search for.\n      value_list: A list of values to match.\n\n    Returns:\n      A list of entries that match the criteria.\n    \"\"\"\n\n    with open(filename, 'r') as f:\n        data = json.load(f)\n\n    matching_entries = []\n    for entry in data.items():\n        #print(entry)\n        if entry[0] in value_list:\n            matching_entries.append(entry)\n\n    return matching_entries\n\n\n# function to download the models with aria2c\ndef download_models_aria2c():\n\n    # Example usage\n    filename = model_reference\n    key = \"name\"\n    value_list = models_to_load\n    # call the function\n    matching_entries = find_matching_entries(filename, key, value_list)\n\n    # download the model(s) and create the hash file(s)\n    print(models_to_load)\n    %cd $worker_path\n    # Print the matching entries\n    for entry in matching_entries:\n        # add civitai api key to the download url\n        if (\"civitai\" in entry[1][\"config\"]['download'][0]['file_url']):\n            entry[1][\"config\"]['download'][0]['file_url'] = entry[1][\"config\"]['download'][0]['file_url'].split(\"?\")[0] + \"?token=\" + civitai_token\n        # extracting the required variables (url, name, sha)\n        # download link\n        # if all 3 outside model variables & match is the mask -> download link = outside model download url\n        if (outside_model and outside_model_name and outside_model_download_url and entry[0] in mask['model_name']):\n            print(f\"Outside model: {outside_model_name}\")\n            download_link = outside_model_download_url\n            # create file to mention the swap\n            swap_file = outside_model_name + \" --- \" + mask['model_name']\n            swap_path = \"/kaggle/working/horde-worker-reGen/compvis/\" + mask['model_name'] + \".outside\"\n            !echo {swap_file} > $swap_path\n        else:\n            download_link = entry[1][\"config\"]['download'][0]['file_url']\n\n        print(download_link)\n        # file name\n        file_name = entry[1][\"config\"]['download'][0]['file_name']\n        print(file_name)\n        file_path = \"compvis/\" + file_name\n        # sha256\n        sha_name = file_name.split(\".\")[0] + \".sha256\"\n        sha = entry[1][\"config\"]['files'][0]['sha256sum']\n        sha = sha + \" *\" + sha_name\n        sha_path = \"/kaggle/working/horde-worker-reGen/compvis/\" + sha_name\n        print(sha)\n        # donwnload model and create sha file\n        !aria2c -c -x 16 -s 16 -p $download_link -o $file_path\n        !rm $sha_path\n        !echo {sha} > $sha_path\n\ndownload_models_aria2c()\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#6.- Run download models\n# Make sure you have the correct path based on any `cd` commands above\n!cd /kaggle/working\n\n!source ../regen/bin/activate;python download_models.py\n","metadata":{"scrolled":true,"id":"nkEiWHVGOAv_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#7.- Run the worker\n# Make sure you have the correct path based on any `cd` commands above\n\nif (outside_model and outside_model_name and outside_model_download_url):\n    Maintenance_Mode = True\nSet_Maintenance_Mode_function(Maintenance_Mode)\n\n!cd /kaggle/working\n\n!source ../regen/bin/activate;python run_worker.py -vv\n","metadata":{"scrolled":true,"id":"vBYrVSGQOAwA","trusted":true},"execution_count":null,"outputs":[]}]}